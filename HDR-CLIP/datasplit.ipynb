{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HRD_file = pd.read_excel(\"DATA_ROOT_DIR/HRD_Score.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           patient ID  HRD_Status\n",
      "476   TCGA-3C-AAAU-01         0.0\n",
      "477   TCGA-3C-AALI-01         0.0\n",
      "478   TCGA-3C-AALJ-01         0.0\n",
      "479   TCGA-3C-AALK-01         0.0\n",
      "480   TCGA-4H-AAAK-01         0.0\n",
      "...               ...         ...\n",
      "1452  TCGA-WT-AB44-01         0.0\n",
      "1453  TCGA-XX-A899-01         0.0\n",
      "1454  TCGA-XX-A89A-01         0.0\n",
      "1455  TCGA-Z7-A8R5-01         0.0\n",
      "1456  TCGA-Z7-A8R6-01         0.0\n",
      "\n",
      "[981 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "HRD_file_BRCA = HRD_file[HRD_file['cancer type']=='BRCA']\n",
    "label = ['patient ID', 'HRD_Status']\n",
    "BRCA_data=pd.DataFrame()\n",
    "for i in label:\n",
    "    BRCA_data[str(i)] = HRD_file_BRCA [i] \n",
    "print(BRCA_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单站点数据生成\n",
    "h5_lists = os.listdir(\"DATA_ROOT_DIR/classification_features_dir/h5_files\")\n",
    "slide_lists = []\n",
    "\n",
    "slide_data = pd.DataFrame()\n",
    "for file in h5_lists:\n",
    "    basename, _ = os.path.splitext(file)\n",
    "    h5_patient = basename[len('slide_'):len('slide_') + 15]\n",
    "    if h5_patient in BRCA_data['patient ID'].values:\n",
    "        BRCA_index = BRCA_data[BRCA_data['patient ID']==h5_patient].index[0]\n",
    "        new_row = pd.DataFrame([{\n",
    "            \"case_id\": h5_patient, \n",
    "            \"slide_id\": basename, \n",
    "            \"censorship\": \"class_\" + str(int(BRCA_data.at[BRCA_index, 'HRD_Status'])), \n",
    "            \"institute\": \"site_0\"\n",
    "        }])\n",
    "        slide_data = pd.concat([slide_data, new_row], ignore_index=True)\n",
    "        # \n",
    "        slide_data.to_csv(\"./dataset_csv/BRCA_nofl_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site_0 count: 507\n",
      "site_1 count: 505\n",
      "site_0 class_0 count: 413\n",
      "site_0 class_1 count: 93\n",
      "site_1 class_0 count: 412\n",
      "site_1 class_1 count: 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lzhao/anaconda/envs/clam_latest/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# 多站点数据生成\n",
    "site_nums = 2\n",
    "h5_lists = os.listdir(\"DATA_ROOT_DIR/classification_features_dir/h5_files\")\n",
    "slide_lists = []\n",
    "\n",
    "slide_data = pd.DataFrame()\n",
    "for file in h5_lists:\n",
    "    basename, _ = os.path.splitext(file)\n",
    "    h5_patient = basename[len('slide_'):len('slide_') + 15]\n",
    "    if h5_patient in BRCA_data['patient ID'].values:\n",
    "        BRCA_index = BRCA_data[BRCA_data['patient ID']==h5_patient].index[0]\n",
    "        new_row = pd.DataFrame([{\n",
    "            \"case_id\": h5_patient, \n",
    "            \"slide_id\": basename, \n",
    "            \"censorship\": \"class_\" + str(int(BRCA_data.at[BRCA_index, 'HRD_Status'])), \n",
    "            \"institute\": \"\"\n",
    "        }])\n",
    "        slide_data = pd.concat([slide_data, new_row], ignore_index=True)\n",
    "# 将slide_data按类别分组\n",
    "grouped = slide_data.groupby('censorship')\n",
    "# 初始化一个空的列表来存储每个站点的数据\n",
    "site_dataframes = [[] for _ in range(site_nums)]\n",
    "# 按类别将数据分配到各个站点\n",
    "for _, group in grouped:\n",
    "    # 将每个类别的数据按站点数量分块\n",
    "    chunks = np.array_split(group, site_nums)\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        site_dataframes[idx].append(chunk)\n",
    "        chunk['institute'] = f\"site_{idx}\"\n",
    "        slide_data = pd.concat([slide_data, chunk], ignore_index=True)\n",
    "# print(slide_data)\n",
    "# 输出当前site_data中site0的个数以及site1的个数\n",
    "site0_count = len(slide_data[slide_data['institute'] == 'site_0'])\n",
    "site1_count = slide_data[slide_data['institute'] == 'site_1'].shape[0]\n",
    "\n",
    "# 输出site0中class_0的个数\n",
    "site0_class0_count = slide_data[(slide_data['institute'] == 'site_0') & (slide_data['censorship'] == 'class_0')].shape[0]\n",
    "site0_class1_count = slide_data[(slide_data['institute'] == 'site_1') & (slide_data['censorship'] == 'class_1')].shape[0]\n",
    "# 输出site1中class_0的个数\n",
    "site1_class0_count = slide_data[(slide_data['institute'] == 'site_1') & (slide_data['censorship'] == 'class_0')].shape[0]\n",
    "site1_class1_count = slide_data[(slide_data['institute'] == 'site_1') & (slide_data['censorship'] == 'class_1')].shape[0]\n",
    "print(f\"site_0 count: {site0_count}\")\n",
    "print(f\"site_1 count: {site1_count}\")\n",
    "print(f\"site_0 class_0 count: {site0_class0_count}\")\n",
    "print(f\"site_0 class_1 count: {site0_class1_count}\")\n",
    "print(f\"site_1 class_0 count: {site1_class0_count}\")\n",
    "print(f\"site_1 class_1 count: {site1_class1_count}\")\n",
    "\n",
    "slide_data.to_csv(\"./dataset_csv/BRCA_fl_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分数据集\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_train_test_valid(input_path, file, site_nums):\n",
    "    res_df = pd.DataFrame() \n",
    "    # read file\n",
    "    df = pd.read_csv(input_path + file)\n",
    "    for site_id in range(site_nums):\n",
    "        train, valid, test =[],[],[]\n",
    "        df['institute'] = df['institute'].str.slice(-1)\n",
    "        df_cur = df[df['institute'] == str(site_id)]['slide_id']\n",
    "        df_cur = df_cur.reset_index(drop=True)\n",
    "        # define the ratios 8:1:1\n",
    "        # Split into train and temp (validation + test)\n",
    "        train, temp = train_test_split(df_cur, test_size=0.2, random_state=42)\n",
    "        train= train.reset_index(drop=True)\n",
    "        # Split temp into validation and test\n",
    "        valid, test = train_test_split(temp, test_size=0.5, random_state=42)\n",
    "        valid= valid.reset_index(drop=True)\n",
    "        test= test.reset_index(drop=True)\n",
    "\n",
    "        tmp ={\"train\":train, \"val\":valid, \"test\" : test}\n",
    "        df_col = pd.DataFrame(tmp)\n",
    "        print(df_col.shape)\n",
    "        # print(df_col)\n",
    "        # df_col.to_csv(\"00.csv\")\n",
    "        res_df = pd.concat([res_df, df_col], ignore_index=True)  \n",
    "\n",
    "    # res_df.to_csv(\"00.csv\") \n",
    "    # print(res_df)\n",
    "    return res_df\n",
    "input_path = \"./dataset_csv/\"\n",
    "file = \"BRCA_fl_data.csv\"\n",
    "site_nums = len(pd.read_csv(input_path + file)['institute'].dropna().unique())\n",
    "print(\"site_nums:\", site_nums)\n",
    "# file = \"BRCA_nofl_data.csv\"\n",
    "split_train_test_valid(input_path, file, site_nums)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site_nums: 1\n",
      "(809, 3)\n",
      "(809, 3)\n",
      "(809, 3)\n",
      "(809, 3)\n",
      "(809, 3)\n"
     ]
    }
   ],
   "source": [
    "input_path = \"./dataset_csv/\"\n",
    "# file = \"classification_hrd_dataset_fl.csv\"\n",
    "# file = \"classification_hrd_dataset.csv\"\n",
    "# file = \"BRCA_fl_data.csv\"\n",
    "file = \"BRCA_nofl_data.csv\"\n",
    "k = 5\n",
    "site_nums = len(pd.read_csv(input_path + file)['institute'].dropna().unique())\n",
    "print(\"site_nums:\", site_nums)\n",
    "for kindex in range(k):\n",
    "    res = split_train_test_valid(input_path, file, site_nums)\n",
    "    if not os.path.exists(\"./splits/fl_classification\"):\n",
    "        os.makedir(\"./splits/fl_classification\")\n",
    "    res.to_csv(\"./splits/fl_classification/splits_{}.csv\".format(kindex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site_nums: 1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'site'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda/envs/clam_latest/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'site'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m res \u001b[38;5;241m=\u001b[39m split_train_test_valid(input_path, file, site_nums)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m res\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m---> 12\u001b[0m     site_name \u001b[38;5;241m=\u001b[39m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     13\u001b[0m     site_data \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# site_data.to_csv(\"00.csv\")\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# print(site_data)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# if not os.path.exists(\"./splits/fl_classification\".format(site_name)):\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m#     os.makedirs(\"./splits/fl_classification/site_{}\".format(site_name))\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda/envs/clam_latest/lib/python3.10/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/anaconda/envs/clam_latest/lib/python3.10/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/anaconda/envs/clam_latest/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'site'"
     ]
    }
   ],
   "source": [
    "input_path = \"./dataset_csv/\"\n",
    "# file = \"classification_hrd_dataset_fl.csv\"\n",
    "# file = \"classification_hrd_dataset.csv\"\n",
    "# file = \"BRCA_fl_data.csv\"\n",
    "file = \"BRCA_nofl_data.csv\"\n",
    "k = 5\n",
    "site_nums = len(pd.read_csv(input_path + file)['institute'].dropna().unique())\n",
    "print(\"site_nums:\", site_nums)\n",
    "for kindex in range(k):\n",
    "    res = split_train_test_valid(input_path, file, site_nums)\n",
    "    for index, row in res.iterrows():\n",
    "        site_name = row['site']\n",
    "        site_data = row['data']\n",
    "        # site_data.to_csv(\"00.csv\")\n",
    "        # print(site_data)\n",
    "        # if not os.path.exists(\"./splits/fl_classification\".format(site_name)):\n",
    "        #     os.makedirs(\"./splits/fl_classification/site_{}\".format(site_name))\n",
    "        if not os.path.exists(\"./splits/nofl_classification\".format(site_name)):\n",
    "            os.makedir(\"./splits/nofl_classification\".format(site_name))\n",
    "        print(kindex, \"次划分:\", site_name, \":train\", len(site_data[\"train\"].dropna()), \",val\", len(site_data[\"val\"].dropna()), \",test\", len(site_data[\"test\"].dropna()))\n",
    "        site_data.to_csv(\"./splits/nofl_classification/splits_{}.csv\".format(kindex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clam_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
